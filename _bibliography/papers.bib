---
---
@article{nguyen2024snoopi,
	title   	 = {Supercharged One-step Text-to-Image Diffusion Models with Negative Prompts},
	author  	 = {Nguyen*, Viet and Nguyen*, Anh and Dao, Trung and Nguyen, Khoi and Pham, Cuong and Tran, Toan and Tran, Anh},
	year    	 = 2025,
	journal 	 = {International Conference on Computer Vision},
	abbr         = {ICCV},
	abstract     = {The escalating demand for real-time image synthesis has driven significant advancements in one-step diffusion models, which inherently offer expedited generation speeds compared to traditional multi-step methods. However, this enhanced efficiency is frequently accompanied by a compromise in the controllability of image attributes. While negative prompting, typically implemented via classifier-free guidance (CFG), has proven effective for fine-grained control in multi-step models, its application to one-step generators remains largely unaddressed. Due to the lack of iterative refinement, as in multi-step diffusion, directly applying CFG to one-step generation leads to blending artifacts and diminished output quality. To fill this gap, we introduce Negative-Away Steer Attention (NASA), an efficient method that integrates negative prompts into one-step diffusion models. NASA operates within the intermediate representation space by leveraging cross-attention mechanisms to suppress undesired visual attributes. This strategy avoids the blending artifacts inherent in output-space guidance and achieves high efficiency, incurring only a minimal 1.89\% increase in FLOPs compared to the computational doubling of CFG. Furthermore, NASA can be seamlessly integrated into existing timestep distillation frameworks, enhancing the student's output quality. Experimental results demonstrate that NASA substantially improves controllability and output quality, achieving an HPSv2 score of 31.21, setting a new state-of-the-art benchmark for one-step diffusion models.},
	arxiv        = {2412.02687},
	pdf          = {https://arxiv.org/pdf/2412.02687.pdf},
	website      = {https://snoopi-onestep.github.io/},
	selected     = {true},
	published    = {true}
}
@article{tran2024dualmodeldefensesafeguardingdiffusion,
	title        = {Dual-Model Defense: Safeguarding Diffusion Models from Membership Inference Attacks through Disjoint Data Splitting},
	author       = {Tran*, Bao and Nguyen*, Viet and Tran, Anh and Tran, Toan},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2410.16657},
	abbr         = {Preprint},
	abstract     = {Diffusion models have demonstrated remarkable capabilities in image synthesis, but their recently proven vulnerability to Membership Inference Attacks (MIAs) poses a critical privacy concern. This paper introduces two novel and efficient approaches (DualMD and DistillMD) to protect diffusion models against MIAs while maintaining high utility. Both methods are based on training two separate diffusion models on disjoint subsets of the original dataset. DualMD then employs a private inference pipeline that utilizes both models. This strategy significantly reduces the risk of black-box MIAs by limiting the information any single model contains about individual training samples. The dual models can also generate "soft targets" to train a private student model in DistillMD, enhancing privacy guarantees against all types of MIAs. Extensive evaluations of DualMD and DistillMD against state-of-the-art MIAs across various datasets in white-box and black-box settings demonstrate their effectiveness in substantially reducing MIA success rates while preserving competitive image generation performance. Notably, our experiments reveal that DistillMD not only defends against MIAs but also mitigates model memorization, indicating that both vulnerabilities stem from overfitting and can be addressed simultaneously with our unified approach.},
	arxiv        = {2410.16657},
	pdf          = {https://arxiv.org/pdf/2410.16657.pdf},
	selected     = {true},
	published    = {true}
}
@article{nguyen2024inference,
	title        = {On Inference Stability for Diffusion Models},
	author       = {Nguyen*, Viet and Vu*, Giang and Nguyen, Thanh Tung and Than, Khoat and Tran, Toan},
	year         = 2024,
	journal      = {AAAI Conference on Artificial Intelligence},
	url          = {https://arxiv.org/abs/2312.12431},
	abbr         = {AAAI},
	abstract     = {Denoising Probabilistic Models (DPMs) represent an emerging domain of generative models that excel in generating diverse and high-quality images. However, most current training methods for DPMs often neglect the correlation between timesteps, limiting the modelâ€™s performance in generating images effectively. Notably, we theoretically point out that this issue can be caused by the cumulative estimation gap between the predicted and the actual trajectory. To minimize that gap, we propose a novel sequence-aware loss that aims to reduce the estimation gap to enhance the sampling quality. Furthermore, we theoretically show that our proposed loss function is a tighter upper bound of the estimation loss in comparison with the conventional loss in DPMs. Experimental results on several benchmark datasets including CIFAR10, CelebA, and CelebA-HQ consistently show a remarkable improvement of our proposed method regarding the image generalization quality measured by FID and Inception Score compared to several DPM baselines. Our code and pre-trained checkpoints are available at https://github.com/VinAIResearch/SA-DPM.},
	award        = {Oral},
	honor        = {Oral Presentation [Top 2\%]},
	pdf          = {https://arxiv.org/pdf/2312.12431.pdf},
	code         = {https://github.com/VinAIResearch/SA-DPM},
	selected     = {true},
	published    = {true}
}
